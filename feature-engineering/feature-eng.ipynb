{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering: A Comprehensive Guide with Examples and ML Terminology\n",
    "\n",
    "In the realm of machine learning (ML) and data science, the process of creating a successful model often hinges on the quality of the features used. This is where feature engineering comes into play â€“ it's the art and science of transforming raw data into informative features that can significantly enhance the performance of ML algorithms.\n",
    "\n",
    "## Key ML Terminology\n",
    "\n",
    "Before we dive into specific techniques, let's clarify some key machine learning terms:\n",
    "\n",
    "1. **Feature**: An individual measurable property or characteristic of a phenomenon being observed. In ML, features are the input variables used to make predictions.\n",
    "\n",
    "2. **Target Variable**: The output variable that the ML model is trying to predict.\n",
    "\n",
    "3. **Supervised Learning**: A type of ML where the model is trained on labeled data, learning to map input features to known output values.\n",
    "\n",
    "4. **Unsupervised Learning**: A type of ML where the model is trained on unlabeled data, trying to find patterns or structures within the data.\n",
    "\n",
    "5. **Overfitting**: When a model learns the training data too well, including its noise and peculiarities, leading to poor generalization on new, unseen data.\n",
    "\n",
    "6. **Underfitting**: When a model is too simple to capture the underlying structure of the data, leading to poor performance on both training and new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from category_encoders import BinaryEncoder, TargetEncoder\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Our Example\n",
    "\n",
    "Let's create a sample dataset to work with. This dataset will include various types of features that we'll use to demonstrate different feature engineering techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "data = pd.DataFrame({\n",
    "    'age': np.random.randint(18, 70, 1000),\n",
    "    'income': np.random.randint(20000, 200000, 1000),\n",
    "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], 1000),\n",
    "    'job_category': np.random.choice(['Entry', 'Mid', 'Senior', 'Executive'], 1000),\n",
    "    'credit_score': np.random.randint(300, 850, 1000),\n",
    "    'has_car': np.random.choice([True, False], 1000),\n",
    "    'favorite_color': np.random.choice(['Blue', 'Red', 'Green', 'Yellow', 'Purple'], 1000)\n",
    "})\n",
    "\n",
    "# Add some missing values\n",
    "data.loc[np.random.choice(data.index, 50), 'credit_score'] = np.nan\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data.head())\n",
    "print(\"\\nData Info:\")\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Creation\n",
    "\n",
    "Feature creation involves generating new features from existing ones. This process often leverages domain knowledge to capture important relationships or characteristics in the data that might not be immediately apparent.\n",
    "\n",
    "Example: Let's create an 'income_per_age' feature, which might capture how efficiently a person earns income relative to their age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['income_per_age'] = data['income'] / data['age']\n",
    "print(\"Feature Creation - income_per_age:\")\n",
    "print(data[['age', 'income', 'income_per_age']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Transformation\n",
    "\n",
    "Feature transformation involves changing the scale or distribution of a feature. This can help in meeting the assumptions of ML algorithms or in revealing non-linear relationships.\n",
    "\n",
    "Example: Let's apply a log transformation to 'income'. This can be useful for handling skewed data or when we expect the impact of income to be multiplicative rather than additive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['log_income'] = np.log(data['income'])\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(121)\n",
    "sns.histplot(data['income'], kde=True)\n",
    "plt.title('Income Distribution')\n",
    "plt.subplot(122)\n",
    "sns.histplot(data['log_income'], kde=True)\n",
    "plt.title('Log Income Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature Transformation - log_income:\")\n",
    "print(data[['income', 'log_income']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handling Categorical Variables\n",
    "\n",
    "Many ML algorithms require numerical input, so we need to convert categorical data into a numerical format. There are several techniques for this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. One-Hot Encoding\n",
    "\n",
    "One-hot encoding creates binary columns for each category. It's suitable for nominal categorical variables with no inherent order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = OneHotEncoder(sparse=False)\n",
    "education_encoded = onehot.fit_transform(data[['education']])\n",
    "education_df = pd.DataFrame(education_encoded, columns=onehot.get_feature_names_out(['education']))\n",
    "print(\"One-Hot Encoding - education:\")\n",
    "print(education_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Label Encoding\n",
    "\n",
    "Label encoding assigns a unique integer to each category. It's suitable for ordinal categorical variables where there's a clear order to the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "data['job_category_encoded'] = le.fit_transform(data['job_category'])\n",
    "print(\"Label Encoding - job_category:\")\n",
    "print(data[['job_category', 'job_category_encoded']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Binary Encoding\n",
    "\n",
    "Binary encoding represents each category as a binary number, then splits the bits into separate columns. This can be more memory-efficient than one-hot encoding for categorical variables with many categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "be = BinaryEncoder(cols=['favorite_color'])\n",
    "color_binary = be.fit_transform(data['favorite_color'])\n",
    "print(\"Binary Encoding - favorite_color:\")\n",
    "print(color_binary.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Target Encoding\n",
    "\n",
    "Target encoding replaces a categorical value with the mean of the target variable for that value. This can be particularly useful when there's a strong relationship between the categorical variable and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TargetEncoder(cols=['education'])\n",
    "data['education_target_encoded'] = te.fit_transform(data['education'], data['income'])\n",
    "print(\"Target Encoding - education (using income as target):\")\n",
    "print(data[['education', 'education_target_encoded']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Scaling\n",
    "\n",
    "Feature scaling is crucial when features have different ranges. It ensures that all features contribute equally to the model's decision-making process. StandardScaler is one common method that standardizes features by removing the mean and scaling to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data['scaled_age'] = scaler.fit_transform(data[['age']])\n",
    "print(\"Feature Scaling - age:\")\n",
    "print(data[['age', 'scaled_age']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Handling Missing Values\n",
    "\n",
    "Missing values can be problematic for many ML algorithms. Imputation is one way to handle this issue. Here, we're using mean imputation, which replaces missing values with the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean')\n",
    "data['credit_score_imputed'] = imputer.fit_transform(data[['credit_score']])\n",
    "\n",
    "print(\"Handling Missing Values - credit_score:\")\n",
    "print(\"Original credit_score:\")\n",
    "print(data['credit_score'].describe())\n",
    "print(\"\\nImputed credit_score:\")\n",
    "print(data['credit_score_imputed'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Selection\n",
    "\n",
    "Feature selection is the process of selecting a subset of relevant features for use in model construction. It can help in reducing overfitting, improving accuracy, and reducing training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['age', 'income', 'credit_score_imputed']]\n",
    "y = data['income_per_age']\n",
    "selector = SelectKBest(score_func=f_regression, k=2)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "selected_features = X.columns[selector.get_support()].tolist()\n",
    "print(\"Feature Selection:\")\n",
    "print(\"Selected features:\", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction techniques can help in reducing the number of features while preserving most of the information. This can be useful for visualization, reducing computational complexity, and mitigating the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a linear dimensionality reduction technique that finds the directions of maximum variance in high-dimensional data and projects it onto a lower dimensional subspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5)\n",
    "plt.title('PCA of Numeric Features')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.show()\n",
    "\n",
    "print(\"PCA:\")\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)"
   ]
  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Truncated SVD\n",
    "\n",
    "Truncated SVD is similar to PCA but can be applied to sparse matrices. It's particularly useful when dealing with high-dimensional data that may be sparse, such as in text processing or collaborative filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "X_svd = svd.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_svd[:, 0], X_svd[:, 1], alpha=0.5)\n",
    "plt.title('Truncated SVD of Numeric Features')\n",
    "plt.xlabel('First SVD Component')\n",
    "plt.ylabel('Second SVD Component')\n",
    "plt.show()\n",
    "\n",
    "print(\"Truncated SVD:\")\n",
    "print(\"Explained variance ratio:\", svd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. t-SNE\n",
    "\n",
    "t-SNE (t-Distributed Stochastic Neighbor Embedding) is a nonlinear dimensionality reduction technique, particularly well suited for the visualization of high-dimensional datasets. Unlike PCA and SVD, t-SNE aims to preserve local structure, making it excellent for visualizing clusters or groups in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.5)\n",
    "plt.title('t-SNE of Numeric Features')\n",
    "plt.xlabel('First t-SNE Component')\n",
    "plt.ylabel('Second t-SNE Component')\n",
    "plt.show()\n",
    "\n",
    "print(\"t-SNE:\")\n",
    "print(\"Note: t-SNE does not provide an explained variance ratio as it's a non-linear method.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this comprehensive guide, we've explored various feature engineering techniques:\n",
    "\n",
    "1. Feature Creation: We created 'income_per_age' to capture earning efficiency.\n",
    "2. Feature Transformation: We applied log transformation to 'income' to handle skewness.\n",
    "3. Handling Categorical Variables: We demonstrated one-hot encoding, label encoding, binary encoding, and target encoding.\n",
    "4. Feature Scaling: We standardized the 'age' feature.\n",
    "5. Handling Missing Values: We used mean imputation for 'credit_score'.\n",
    "6. Feature Selection: We selected the most relevant features using SelectKBest.\n",
    "7. Dimensionality Reduction: We explored PCA, Truncated SVD, and t-SNE for reducing feature space and visualization.\n",
    "\n",
    "Each of these techniques serves a specific purpose in preparing data for machine learning models. The choice of which techniques to use depends on your specific dataset, the nature of your features, and the requirements of your chosen machine learning algorithm.\n",
    "\n",
    "Remember, feature engineering is both an art and a science. It requires creativity, domain knowledge, and a good understanding of your data and the problem you're trying to solve. Experimentation and iteration are key to finding the best set of features for your specific machine learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final processed data\n",
    "print(\"Final Processed Data:\")\n",
    "print(data.head())\n",
    "print(\"\\nFinal Data Info:\")\n",
    "print(data.info())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
 }