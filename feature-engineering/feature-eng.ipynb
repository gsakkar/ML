{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comprehensive Feature Engineering Tutorial\n",
        "\n",
        "This notebook demonstrates various feature engineering techniques using a sample dataset. We'll go through each step of the process, explaining the concepts and showing their effects on the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# First, let's import the necessary libraries. \n",
        "# pandas is used for data manipulation and analysis.\n",
        "# numpy is used for numerical operations.\n",
        "# sklearn provides machine learning tools.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating the Sample Dataset\n",
        "\n",
        "Let's create a sample dataset to work with. This dataset represents information about individuals, including their age, income, education level, car ownership, and credit score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "data = pd.DataFrame({\n",
        "    'age': [25, 30, 35, 40, 45, 50, 55, 60],\n",
        "    'income': [30000, 45000, 50000, 60000, 70000, 80000, 85000, 90000],\n",
        "    'education': ['High School', 'Bachelor', 'Master', 'PhD', 'Bachelor', 'Master', 'High School', 'PhD'],\n",
        "    'has_car': [True, False, True, True, False, True, False, True],\n",
        "    'credit_score': [650, 700, np.nan, 800, 750, np.nan, 600, 850]\n",
        "})\n",
        "\n",
        "print(\"Original Data:\")\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Feature Creation\n",
        "\n",
        "Feature creation involves generating new features from existing ones. Here, we're creating a new feature 'income_per_age' by dividing income by age. This could potentially capture how income changes with age."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "data['income_per_age'] = data['income'] / data['age']\n",
        "print(\"\\n1. Feature Creation\")\n",
        "print(data['income_per_age'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Feature Transformation\n",
        "\n",
        "Feature transformation involves changing the scale or distribution of a feature. Here, we're applying a logarithmic transformation to 'income'. This can be useful for handling skewed data or when we expect the impact of income to be multiplicative rather than additive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "data['log_income'] = np.log(data['income'])\n",
        "print(\"\\n2. Feature Transformation\")\n",
        "print(data['log_income'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Handling Categorical Variables\n",
        "\n",
        "Machine learning models typically work with numerical data. One-hot encoding is a method to convert categorical variables into a form that could be provided to ML algorithms to do a better job in prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "encoder = OneHotEncoder(sparse=False)\n",
        "education_encoded = encoder.fit_transform(data[['education']])\n",
        "education_df = pd.DataFrame(education_encoded, columns=encoder.get_feature_names_out(['education']))\n",
        "data = pd.concat([data, education_df], axis=1)\n",
        "print(\"\\n3. Handling Categorical Variables\")\n",
        "print(data[encoder.get_feature_names_out(['education'])])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature Scaling\n",
        "\n",
        "Feature scaling is a method used to standardize the range of independent variables or features of data. StandardScaler standardizes features by removing the mean and scaling to unit variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "scaler = StandardScaler()\n",
        "data['scaled_age'] = scaler.fit_transform(data[['age']])\n",
        "print(\"\\n4. Feature Scaling\")\n",
        "print(data['scaled_age'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Handling Missing Values\n",
        "\n",
        "Missing values can be problematic for many machine learning algorithms. Imputation is the process of replacing missing data with substituted values. Here, we're using mean imputation, which replaces missing values with the mean of the column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "imputer = SimpleImputer(strategy='mean')\n",
        "data['credit_score_imputed'] = imputer.fit_transform(data[['credit_score']])\n",
        "print(\"\\n5. Handling Missing Values\")\n",
        "print(data['credit_score_imputed'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature Selection\n",
        "\n",
        "Feature selection is the process of selecting a subset of relevant features for use in model construction. SelectKBest selects features according to the k highest scores. Here, we're using f_regression which computes the F-value between label/feature for regression tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "X = data[['age', 'income', 'credit_score_imputed']]\n",
        "y = data['income_per_age']\n",
        "selector = SelectKBest(score_func=f_regression, k=2)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "selected_features = X.columns[selector.get_support()].tolist()\n",
        "print(\"\\n6. Feature Selection\")\n",
        "print(\"Selected features:\", selected_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Dimensionality Reduction\n",
        "\n",
        "Dimensionality reduction is the process of reducing the number of random variables under consideration. Principal Component Analysis (PCA) is a technique used to emphasize variation and bring out strong patterns in a dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "print(\"\\n7. Dimensionality Reduction\")\n",
        "print(\"PCA components shape:\", X_pca.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Processed Data\n",
        "\n",
        "Let's take a look at our final processed dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "print(\"\\nFinal Processed Data:\")\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This final dataset includes all our original features, plus:\n",
        "- The created feature (income_per_age)\n",
        "- The transformed feature (log_income)\n",
        "- One-hot encoded education levels\n",
        "- Scaled age\n",
        "- Imputed credit score\n",
        "\n",
        "We've also performed feature selection and dimensionality reduction, which can be used to choose which features to include in our final model.\n",
        "\n",
        "Remember, the choice of which feature engineering techniques to use depends on your specific dataset and the requirements of your machine learning model."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}